# GPU Optimization Guide for Batch Segmentation

This guide explains how to maximize T4 GPU utilization for batch dental image segmentation.

## Quick Start

### Using the Optimized Script

```bash
# Basic usage with default settings (optimized for T4)
python batch_segment_optimized.py /path/to/data --device cuda

# With custom SAM batch size
python batch_segment_optimized.py /path/to/data --device cuda --sam-batch-size 64

# Profile to find optimal settings
python gpu_profile_batch.py --test-image /path/to/test/image.png --benchmark-sam --profile-pipeline
```

## Key Optimizations Implemented

### 1. Model Reuse (`batch_segment_optimized.py`)

**Problem:** Original code loaded models for every image
- SAM model: ~40MB, loaded 1000s of times
- YOLO models: ~6MB each, loaded repeatedly

**Solution:** `OptimizedSegmenter` class
- Loads models once at initialization
- Caches YOLO models per view
- Models stay on GPU throughout processing

**Impact:** 10-50x faster initialization per image

### 2. Explicit GPU Placement

**Problem:** Models defaulted to CPU

**Solution:**
```python
self.sam = sam_load(model_path).to('cuda')
if self.device == 'cuda':
    yolo_model.to('cuda')
```

**Impact:** Enables GPU acceleration entirely

### 3. Larger SAM Batch Sizes

**Problem:** Default batch size of 10 underutilizes T4 GPU

**Solution:** Configurable batch size (default: 32)
```bash
--sam-batch-size 32  # Default, safe for T4
--sam-batch-size 64  # Try if you have headroom
```

**Impact:** Better GPU utilization, higher throughput

### 4. Pinned Memory for Faster Transfers

**Problem:** CPU‚ÜíGPU transfers are slow

**Solution:**
```python
boxes_tensor = torch.tensor(boxes).pin_memory()
transformed_boxes.to(device, non_blocking=True)
```

**Impact:** ~20% faster data transfer

### 5. cuDNN Autotuning

**Problem:** PyTorch uses conservative defaults

**Solution:**
```python
torch.backends.cudnn.benchmark = True  # Find fastest algorithms
torch.backends.cuda.matmul.allow_tf32 = True  # Use TensorFloat-32
```

**Impact:** 10-30% speedup on convolutions

## Performance Tuning Guide

### Step 1: Profile Your Workload

```bash
python gpu_profile_batch.py \
    --test-image /path/to/typical/image.png \
    --benchmark-sam \
    --profile-pipeline \
    --view front
```

This will:
- Test different SAM batch sizes
- Identify bottlenecks (I/O vs compute)
- Show GPU utilization and memory usage

### Step 2: Interpret Results

#### Example Output:

```
SAM Batch Size Benchmark
========================================
Testing with 20 detections:
  Batch size   4: 0.245s (81.6 boxes/s) | GPU:  45% | Mem: 2.1GB (28%)
  Batch size  16: 0.156s (128.2 boxes/s) | GPU:  72% | Mem: 2.8GB (37%)
  Batch size  32: 0.142s (140.8 boxes/s) | GPU:  88% | Mem: 3.4GB (45%)  ‚Üê Optimal
  Batch size  64: 0.139s (143.9 boxes/s) | GPU:  91% | Mem: 5.2GB (69%)
  Batch size 128: OOM (out of memory)

Bottleneck Analysis:
  I/O time:        15.3%
  GPU compute:     84.7%  ‚Üê GPU-bound (good!)
```

**Interpretation:**
- Batch size 32 gives best performance without excessive memory
- GPU utilization is high (88%)
- Compute-bound (not I/O-bound) means GPU is working hard

#### If You See Low GPU Utilization (<50%):

```
  I/O time:        65.2%  ‚Üê I/O bottleneck!
  GPU compute:     34.8%
```

**Solutions:**
1. Use faster storage (SSD/NVMe)
2. Increase SAM batch size
3. Consider multi-threaded I/O (future enhancement)

### Step 3: Choose Optimal Batch Size

Based on profiling:

| T4 Memory | Typical Teeth | Recommended Batch Size |
|-----------|---------------|------------------------|
| 15GB      | 10-15         | 32-64                  |
| 15GB      | 20-30         | 32-48                  |
| 15GB      | 30+           | 16-32                  |

**Rule of thumb:**
- Start with 32
- Increase if GPU util < 70%
- Decrease if you hit OOM

### Step 4: Run Optimized Batch Processing

```bash
python batch_segment_optimized.py \
    /path/to/data \
    --device cuda \
    --sam-batch-size 32 \
    --conf-threshold 0.01
```

## Expected Performance

### T4 GPU Specifications
- 16GB GDDR6 memory
- 8.1 TFLOPS FP32
- 260 GB/s memory bandwidth
- 70W TDP

### Typical Performance (T4)

| Configuration | Images/sec | GPU Util | Memory |
|--------------|------------|----------|---------|
| Original code | 0.5-1.0   | 20-30%   | 2-3GB   |
| Optimized (batch=16) | 2-3 | 60-70% | 3-4GB |
| Optimized (batch=32) | 3-4 | 80-90% | 4-5GB |
| Optimized (batch=64) | 3.5-5 | 85-95% | 6-8GB |

*Actual performance depends on image resolution, teeth count, and I/O speed*

## Advanced Optimizations

### 1. Mixed Precision (FP16)

For even faster inference, use half precision:

```python
# Add to OptimizedSegmenter.__init__
if self.device == 'cuda':
    self.sam = self.sam.half()  # Convert to FP16
```

**Benefit:** ~2x faster, 50% less memory
**Tradeoff:** Slightly reduced accuracy (usually negligible)

### 2. Compile Models (PyTorch 2.0+)

```python
self.sam = torch.compile(self.sam, mode='max-autotune')
```

**Benefit:** 10-30% speedup
**Tradeoff:** Slower first run (compilation time)

### 3. Pre-load Images to RAM

For datasets that fit in RAM:

```python
# Load all images once
image_cache = {path: cv2.imread(str(path)) for path in image_paths}
```

**Benefit:** Eliminates I/O bottleneck
**Tradeoff:** High RAM usage

### 4. Multi-GPU Support (Future)

For multiple GPUs:

```python
# Distribute processing across GPUs
torch.nn.DataParallel(model, device_ids=[0, 1])
```

## Monitoring GPU During Processing

### Using nvidia-smi

```bash
# Watch GPU usage in real-time
watch -n 1 nvidia-smi

# Log GPU metrics
nvidia-smi --query-gpu=timestamp,utilization.gpu,utilization.memory,memory.used,memory.total,power.draw,temperature.gpu --format=csv -l 1 > gpu_log.csv
```

### Using Python (pynvml)

```bash
# Install monitoring library
pip install nvidia-ml-py3

# Use the profiler
python gpu_profile_batch.py --test-image /path/to/image.png --profile-pipeline
```

## Troubleshooting

### Out of Memory (OOM)

**Symptoms:**
```
torch.cuda.OutOfMemoryError: CUDA out of memory
```

**Solutions:**
1. Reduce SAM batch size: `--sam-batch-size 16`
2. Clear cache: Add `torch.cuda.empty_cache()` calls
3. Use smaller YOLO conf threshold (fewer detections)

### Low GPU Utilization

**Symptoms:** GPU util < 50% in nvidia-smi

**Causes & Solutions:**
1. **I/O bottleneck:** Use SSD, reduce image resolution, or pre-load images
2. **Small batch size:** Increase `--sam-batch-size`
3. **CPU preprocessing:** Minimize OpenCV operations before GPU transfer

### Slow First Image

**Symptoms:** First image takes 10x longer

**Cause:** Model loading + CUDA initialization

**Solution:** This is expected; subsequent images will be fast

### Models Not Using GPU

**Symptoms:** Processing slow, `nvidia-smi` shows 0% util

**Check:**
```bash
python -c "import torch; print(torch.cuda.is_available())"
```

If False:
- Install CUDA-enabled PyTorch: `pip install torch --index-url https://download.pytorch.org/whl/cu118`
- Check NVIDIA drivers: `nvidia-smi`

## Comparison: Original vs Optimized

| Metric | Original | Optimized | Improvement |
|--------|----------|-----------|-------------|
| Models loaded per image | 2 | 0 (cached) | ‚àû |
| GPU device | CPU default | Explicit CUDA | ‚àû |
| SAM batch size | 10 | 32 (tunable) | 3.2x |
| Memory optimization | None | Pinned | ~1.2x |
| cuDNN tuning | Off | On | ~1.2x |
| **Overall speedup** | 1x | **10-20x** | **üöÄ** |

## Additional Resources

- [PyTorch Performance Tuning Guide](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html)
- [NVIDIA T4 Datasheet](https://www.nvidia.com/en-us/data-center/tesla-t4/)
- [Ultralytics GPU Optimization](https://docs.ultralytics.com/guides/speed-optimization/)

## Summary Commands

```bash
# 1. Profile to find optimal settings
python gpu_profile_batch.py --test-image sample.png --benchmark-sam

# 2. Run optimized batch processing
python batch_segment_optimized.py /data --device cuda --sam-batch-size 32

# 3. Monitor GPU during processing
watch -n 1 nvidia-smi
```

## Questions or Issues?

If GPU utilization is still low after optimization, check:
1. Image loading speed (try pre-loading)
2. Disk I/O (use `iotop` on Linux)
3. YOLO detection count (more detections = more GPU work)
4. T4 power limits (ensure not throttled)
